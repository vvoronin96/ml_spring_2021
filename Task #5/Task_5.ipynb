{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task #5.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "98O3Zdco6tiS"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTvxnyOW08SI",
        "outputId": "3be1cb2e-3478-443a-e968-e6b27cb0b472"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "from nltk.collocations import *\r\n",
        "from nltk.stem import *\r\n",
        "from nltk.corpus import stopwords\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('nps_chat')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]   Package nps_chat is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-dzq0Uw1Mqx"
      },
      "source": [
        "\"\"\"\r\n",
        "return text as array of lines\r\n",
        "\"\"\"\r\n",
        "def read_book(title_path): \r\n",
        "\twith open(title_path, \"r\", encoding =\"utf8\") as current_file: \r\n",
        "\t\ttext = current_file.readlines()\r\n",
        "\treturn text\r\n",
        "\r\n",
        "def divide_into_chapters(text):\r\n",
        "  \"\"\"\r\n",
        "  return array of chapter presenting as array of lines\r\n",
        "  \"\"\"\r\n",
        "  string_number = 0\r\n",
        "  content_list = []\r\n",
        "  chapters = []\r\n",
        "\r\n",
        "  for string in text:\r\n",
        "    string_number+=1\r\n",
        "    if re.match('CHAPTER', string):\r\n",
        "      content_list.append(string_number)\r\n",
        "      continue\r\n",
        "    if re.match('THE END', string):\r\n",
        "      content_list.append(string_number)\r\n",
        "  \r\n",
        "  for i in range(0, len(content_list)-1):\r\n",
        "    start = content_list[i] \r\n",
        "    finish = content_list[i+1] - 1 #w/o one line at the end of chapter, because not to cut the beginning of next chapter\r\n",
        "    chapters.append(text[start:finish])  \r\n",
        "  return chapters\r\n",
        "\r\n",
        "def get_sentences(chapter):\r\n",
        "  sentences = []\r\n",
        "  sentence = \"\"\r\n",
        "  for i in range(len(chapter)):\r\n",
        "    if chapter[i] == \"\\n\":\r\n",
        "      sentences.append(sentence)\r\n",
        "      sentence = \"\"\r\n",
        "    else:\r\n",
        "      sentence += chapter[i]\r\n",
        "      if i == len(chapter)-1:\r\n",
        "        sentences.append(sentence)\r\n",
        "  return sentences \r\n",
        "\r\n",
        "def clean_text(sentence, alice_only = False):\r\n",
        "  sentence = re.sub(r'\\n', ' ', sentence)\r\n",
        "  sentence = sentence.lower()\r\n",
        "  sentence = re.sub(\"<.*?\\>\", \" \", sentence)\r\n",
        "  sentence = re.sub(r'_', \" \", sentence)\r\n",
        "  sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence, re.UNICODE)\r\n",
        "  sentence = re.sub(r'\\s+', \" \", sentence)\r\n",
        "  if alice_only and \"alice\" in sentence:\r\n",
        "    sentence = re.sub(\"alice\", \"\", sentence)\r\n",
        "    return sentence #return only cleaned sentences with Alice\r\n",
        "  else:\r\n",
        "    sentence = re.sub(\"alice\", \"\", sentence) #otherwise deletion \"Alice\" from sentences is as step of cleaning procedure\r\n",
        "  if alice_only == False:\r\n",
        "    return sentence\r\n",
        "\r\n",
        "def get_alice_action(chapter):\r\n",
        "  tokens = [nltk.word_tokenize(sentence) for sentence in chapter]\r\n",
        "  \r\n",
        "  #define part-of-spitch \r\n",
        "  pos_list = [nltk.pos_tag(word) for word in tokens]\r\n",
        "  \r\n",
        "  #select only verbs from sentence \r\n",
        "  alice_action = [[word for (word, tags) in i if tags in ('VB', 'VBZ')] for i in pos_list]\r\n",
        "  \r\n",
        "  #transform alice_action array into string and return it \r\n",
        "  alice_action = [\" \".join(i) for i in alice_action]\r\n",
        "  alice_action = \" \".join(alice_action)\r\n",
        "  \r\n",
        "  return alice_action"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHokorODELen"
      },
      "source": [
        "##Top 10 important words in each chapter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDJSl3J3T-yT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d24d762-df41-496d-8e81-51fe21870a5d"
      },
      "source": [
        "text = read_book('/content/drive/My Drive/Datasets/11-0.txt')\r\n",
        "chapters = divide_into_chapters(text)\r\n",
        "\r\n",
        "#print(\"Before normalization: \", chapters[8])\r\n",
        "for i in range(len(chapters)):\r\n",
        "  chapters[i] = \"\".join(chapters[i])\r\n",
        "  chapters[i] = clean_text(chapters[i])\r\n",
        "#print(\"After normalization: \", chapters[8],\"\\n\\n\")  \r\n",
        "\r\n",
        "#run vectorixer which computes the word counts, idf and tf-idf values all at once and also excepts stop-words\r\n",
        "vectorizer = TfidfVectorizer(use_idf=True, stop_words='english')\r\n",
        "X = vectorizer.fit_transform(chapters)\r\n",
        "\r\n",
        "#create table [word][tf-idf value] for each chapters in book\r\n",
        "#sort descent by [tf-idf value] and show the first 10 values\r\n",
        "print(\"Top 10 important words each chapter\")\r\n",
        "for i in range(len(chapters)):\r\n",
        "  df = pd.DataFrame(X[i].T.todense(), index=vectorizer.get_feature_names(), columns=[\"Chapter {0}\".format(i+1)])\r\n",
        "  print((df.sort_values(by=[\"Chapter {0}\".format(i+1)],ascending=False))[:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 important words each chapter\n",
            "        Chapter 1\n",
            "little   0.183798\n",
            "key      0.160173\n",
            "way      0.159291\n",
            "eat      0.152091\n",
            "rabbit   0.145618\n",
            "bats     0.140755\n",
            "like     0.134785\n",
            "think    0.134785\n",
            "door     0.127415\n",
            "bottle   0.120882\n",
            "        Chapter 2\n",
            "mouse    0.364099\n",
            "pool     0.195979\n",
            "little   0.191153\n",
            "cats     0.171482\n",
            "oh       0.170595\n",
            "swam     0.161457\n",
            "ll       0.145732\n",
            "dear     0.141944\n",
            "said     0.134931\n",
            "mabel    0.129165\n",
            "         Chapter 3\n",
            "said      0.378755\n",
            "mouse     0.360718\n",
            "dodo      0.329696\n",
            "lory      0.164848\n",
            "race      0.159957\n",
            "prizes    0.159957\n",
            "dry       0.145620\n",
            "thimble   0.127966\n",
            "know      0.122539\n",
            "dinah     0.108921\n",
            "         Chapter 4\n",
            "little    0.220710\n",
            "window    0.220464\n",
            "rabbit    0.213826\n",
            "puppy     0.192906\n",
            "gloves    0.165670\n",
            "chimney   0.165348\n",
            "bottle    0.142003\n",
            "fan       0.142003\n",
            "said      0.134345\n",
            "room      0.119110\n",
            "             Chapter 5\n",
            "caterpillar   0.491890\n",
            "said          0.451554\n",
            "pigeon        0.299256\n",
            "serpent       0.224442\n",
            "youth         0.149628\n",
            "eggs          0.124690\n",
            "father        0.107085\n",
            "size          0.101887\n",
            "little        0.095521\n",
            "hookah        0.085668\n",
            "         Chapter 6\n",
            "said      0.388149\n",
            "cat       0.322165\n",
            "footman   0.284601\n",
            "baby      0.244418\n",
            "mad       0.215909\n",
            "duchess   0.171753\n",
            "pig       0.143939\n",
            "wow       0.142300\n",
            "like      0.132136\n",
            "cook      0.125947\n",
            "          Chapter 7\n",
            "hatter     0.464200\n",
            "dormouse   0.429951\n",
            "said       0.380938\n",
            "march      0.277770\n",
            "hare       0.265144\n",
            "twinkle    0.148336\n",
            "tea        0.138885\n",
            "time       0.116219\n",
            "draw       0.095545\n",
            "treacle    0.095545\n",
            "             Chapter 8\n",
            "queen         0.468794\n",
            "said          0.355734\n",
            "king          0.210310\n",
            "gardeners     0.190065\n",
            "soldiers      0.183633\n",
            "hedgehog      0.166306\n",
            "cat           0.161363\n",
            "executioner   0.142548\n",
            "procession    0.118790\n",
            "game          0.102686\n",
            "         Chapter 9\n",
            "turtle    0.425172\n",
            "said      0.411991\n",
            "mock      0.409425\n",
            "gryphon   0.282685\n",
            "duchess   0.204005\n",
            "moral     0.166057\n",
            "queen     0.163832\n",
            "went      0.093963\n",
            "school    0.089132\n",
            "did       0.079507\n",
            "           Chapter 10\n",
            "turtle       0.397869\n",
            "mock         0.384150\n",
            "gryphon      0.381747\n",
            "said         0.283378\n",
            "dance        0.235099\n",
            "soup         0.164636\n",
            "join         0.162761\n",
            "beautiful    0.150916\n",
            "whiting      0.144677\n",
            "lobster      0.126592\n",
            "          Chapter 11\n",
            "king        0.411349\n",
            "hatter      0.370155\n",
            "said        0.323621\n",
            "court       0.299309\n",
            "dormouse    0.259401\n",
            "witness     0.232344\n",
            "jury        0.123385\n",
            "queen       0.117890\n",
            "butter      0.099770\n",
            "bread       0.099770\n",
            "           Chapter 12\n",
            "said         0.474098\n",
            "king         0.399928\n",
            "jury         0.202529\n",
            "sister       0.160490\n",
            "queen        0.150507\n",
            "important    0.141770\n",
            "dream        0.137563\n",
            "rabbit       0.110475\n",
            "verses       0.106786\n",
            "jurymen      0.106786\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EK64S29ER2u"
      },
      "source": [
        "##Top 10 what does Alice like to do "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6UIbUYzjBnm"
      },
      "source": [
        "To calculate TF-IDF metric it supposes that the document (s) is one chapter of the book (D), the term (i) is word in this chapter. \r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "leXt-2P9U_7V",
        "outputId": "91fea3ff-5a79-4d80-8559-3b0669993468"
      },
      "source": [
        "a = [[\"test test2\"], [\"test3 test4\"]]\r\n",
        "b = \" \".join(a)\r\n",
        "print(b)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-682ca19d3a86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test test2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"test3 test4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, list found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4NZZhHEYiML",
        "outputId": "f96fcb7b-28c1-429b-b618-9175c5711922"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "# list of text documents\r\n",
        "text = [\"The quick brown fox jumped brown over the lazy dog.\"]\r\n",
        "# create the transform\r\n",
        "vectorizer = CountVectorizer()\r\n",
        "# tokenize and build vocab\r\n",
        "vectorizer.fit(text)\r\n",
        "# summarize\r\n",
        "print(vectorizer.vocabulary_)\r\n",
        "# encode document\r\n",
        "vector = vectorizer.transform(text)\r\n",
        "# summarize encoded vector\r\n",
        "print(\"vector shape\", vector.shape)\r\n",
        "print(type(vector))\r\n",
        "print(vector.toarray())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
            "vector shape (1, 8)\n",
            "<class 'scipy.sparse.csr.csr_matrix'>\n",
            "[[2 1 1 1 1 1 1 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbjwL9cupizO",
        "outputId": "b80167bc-7bba-49b8-c3e5-ff55305b2f7d"
      },
      "source": [
        "text = read_book('/content/drive/My Drive/Datasets/11-0.txt')\r\n",
        "chapters = divide_into_chapters(text)\r\n",
        "chapters = [get_sentences(chapter) for chapter in chapters]\r\n",
        "\r\n",
        "for i in range(len(chapters)):\r\n",
        "  #select only sentences with Alice\r\n",
        "  chapters[i] = [clean_text(sentence, True) for sentence in chapters[i]]\r\n",
        "  #sentences w/o Alice give None, so delete its\r\n",
        "  chapters[i] = list(filter(None, chapters[i]))\r\n",
        "\r\n",
        "alice_action_in_book = [get_alice_action(chapter) for chapter in chapters]\r\n",
        "temp = []\r\n",
        "temp = \" \".join(alice_action_in_book)\r\n",
        "alice_action_in_book.clear()\r\n",
        "alice_action_in_book.append(temp)\r\n",
        "\r\n",
        "#run vectorixer which computes the word counts, idf and tf-idf values all at once and also excepts stop-words\r\n",
        "vectorizer = CountVectorizer(stop_words='english')\r\n",
        "X = vectorizer.fit_transform(alice_action_in_book)\r\n",
        "\r\n",
        "#create table [word][tf-idf value] for each chapters in book\r\n",
        "#sort descent by [tf-idf value] and show the first 10 values\r\n",
        "for i in range(len(alice_action_in_book)):\r\n",
        "  df = pd.DataFrame(X[i].T.todense(), index=vectorizer.get_feature_names(), columns=[\"Alice action in book\"])\r\n",
        "  print((df.sort_values(by=[\"Alice action in book\"],ascending=False))[:10])\r\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          Alice action in book\n",
            "say                         20\n",
            "think                       18\n",
            "make                        16\n",
            "tell                        13\n",
            "come                        12\n",
            "know                        12\n",
            "eat                         10\n",
            "look                         9\n",
            "remember                     9\n",
            "hear                         9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98O3Zdco6tiS"
      },
      "source": [
        "##Draft section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "hvc4ULDsSG7z",
        "outputId": "8f6a41a0-7ee0-4458-ef35-b9b5c6a1019d"
      },
      "source": [
        "#лемматизация\r\n",
        "stemmer = PorterStemmer()\r\n",
        "lemmatizer = WordNetLemmatizer()\r\n",
        "w = []\r\n",
        "for i in range(len(tokens)):\r\n",
        "  w.append([lemmatizer.lemmatize(q) for q in tokens[i]])\r\n",
        "\r\n",
        "print(\"w\", w)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-881bf05925d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v7VAcP5_j1d"
      },
      "source": [
        "#делит книгу на главы, главы на предложения, чистит их и возвращает главу как массив очищенных предложений\r\n",
        "def get_sentences(chapter):\r\n",
        "  sentences = []\r\n",
        "  sentence = \"\"\r\n",
        "  for i in range(len(chapter)):\r\n",
        "    if chapter[i] == \"\\n\":\r\n",
        "      sentences.append(sentence)\r\n",
        "      sentence = \"\"\r\n",
        "    else:\r\n",
        "      sentence += chapter[i]\r\n",
        "      if i == len(chapter)-1:\r\n",
        "        sentences.append(sentence)\r\n",
        "  return sentences \r\n",
        "\r\n",
        "def clean_text(sentence):\r\n",
        "  sentence = re.sub(r'\\n', ' ', sentence)\r\n",
        "  sentence = sentence.lower()\r\n",
        "  sentence = re.sub(\"<.*?\\>\", \" \", sentence)\r\n",
        "  sentence = re.sub(\"alice\", \"\", sentence)\r\n",
        "  sentence = re.sub(r'_', \" \", sentence)\r\n",
        "  sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence, re.UNICODE)\r\n",
        "  sentence = re.sub(r'\\s+', \" \", sentence)\r\n",
        "  \r\n",
        "  return sentence\r\n",
        "   \r\n",
        "text = read_book('/content/drive/My Drive/Datasets/11-0.txt')\r\n",
        "chapters = divide_into_chapters(text)\r\n",
        "print(\"Before: \", chapters[8])\r\n",
        "for i in range(len(chapters)):\r\n",
        "  chapters[i] = [clean_text(sentence) for sentence in get_sentences(chapters[i])]\r\n",
        "  chapters[i] = [w for w in chapters[i] if w != \"\"] \r\n",
        "print(\"After: \", chapters[8])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgFaIqBl6mDA"
      },
      "source": [
        "полезная ссылка про векторизацию текста\r\n",
        "https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.YC5hAWgzbBU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQO1k-EMSDQw"
      },
      "source": [
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\r\n",
        "text = \"I do not like green eggs and ham, I do not like them Sam I am!\"\r\n",
        "tokens = nltk.wordpunct_tokenize(text)\r\n",
        "finder = BigramCollocationFinder.from_words(tokens)\r\n",
        "scored = finder.score_ngrams(bigram_measures.raw_freq)\r\n",
        "sorted(bigram for bigram, score in scored)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}